# -*- coding: utf-8 -*-
"""Creating ChatGpt replica.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cgBCECI2TIhK3qiqAQeD96OrwEGjccHv
"""

!pip install -qU langchain langchain-openai langchain-google-genai
# the libraries installed here are langchain, langchain-openai, langchain-google-genai

"""# **Storing API**



*   Get OpenAI API key: https://platform.openai.com/account/api-keys


"""

import os
from getpass import getpass
# import secrets

os.environ['OPENAI_API_KEY'] = getpass('Enter your OpenAI api key: ')

"""  * Get Google AI API key: https://ai.google.dev/"""

os.environ['GOOGLE_API_KEY'] = getpass('Enter your Google API Key: ')

"""# **Using models from OpenAI and Google**

"""

# Using OpenAI Models ( GPT 3.5)

from langchain_openai import ChatOpenAI

gpt3_model = ChatOpenAI(model = "gpt-3.5-turbo-0125", temperature = 0.0)


# Using Google Models ( Gemini Pro )
from langchain_google_genai import ChatGoogleGenerativeAI

gemini_model = ChatGoogleGenerativeAI(model = "gemini-pro")

# Example for gpt model
gpt3_model.invoke(' who is the first black president of USA?')

gemini_model.invoke('who is the first African american president of USA?')
# this doesn't work because of Gemini restrictions against racism (word: black), safety restrictions

"""Adding memory to the chain"""

from langchain.chat_models import ChatOpenAI
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory

memory = ConversationBufferMemory(k=30) # k = 30 implies, 30 previous Queries and Responses are atmost remembered

# ConversationSummaryMemory(k=3) -> instead of the same Query and Response forwarded to next Query ,
# it passes the summary of the every k = 3 conversations to the next query
conversation = ConversationChain( llm = gpt3_model, # ChatOpenAi
                                 memory = memory)  # memory

conversation.predict(input='Who is the first black president of USA')

conversation.predict(input = 'When was he born')

# previous Query and Response are added in this too before this query is executed
# so in total Q1R1 + Q2 => R2

conversation.predict(input = 'When did his tenure end')